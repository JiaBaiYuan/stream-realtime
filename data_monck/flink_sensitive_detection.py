#!/usr/bin/env python3
# coding: utf-8

import os
import json
import logging
from pyflink.datastream import StreamExecutionEnvironment, FlatMapFunction, RuntimeContext
from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common import WatermarkStrategy
from pyflink.datastream.connectors.jdbc import JdbcSink, JdbcConnectionOptions, JdbcExecutionOptions
from pyflink.common import Types
import jpype
import jpype.imports
from jpype.types import *

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SensitiveWordDetector(FlatMapFunction):
    """
    Flink FlatMapFunction ç”¨äºæ£€æµ‹æ•æ„Ÿè¯„è®º
    """

    def __init__(self, sensitive_words_file="../resource/data/sensitiveword/suspected-sensitive-words.txt"):
        self.sensitive_words_file = sensitive_words_file
        self.sensitive_words = set()

    def open(self, runtime_context: RuntimeContext):
        """åˆå§‹åŒ–æ•æ„Ÿè¯åº“"""
        logger.info("æ­£åœ¨åŠ è½½æ•æ„Ÿè¯åº“...")
        try:
            with open(self.sensitive_words_file, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip()
                    if word and len(word) > 1:  # è¿‡æ»¤ç©ºè¡Œå’Œå•å­—
                        self.sensitive_words.add(word)
                        # åŒæ—¶æ·»åŠ å°å†™ç‰ˆæœ¬
                        self.sensitive_words.add(word.lower())

            logger.info(f"æ•æ„Ÿè¯åº“åŠ è½½å®Œæˆï¼Œå…± {len(self.sensitive_words)} ä¸ªæ•æ„Ÿè¯")
        except Exception as e:
            logger.error(f"åŠ è½½æ•æ„Ÿè¯åº“å¤±è´¥: {e}")
            # ä½¿ç”¨ä¸€äº›é»˜è®¤æ•æ„Ÿè¯ä½œä¸ºå¤‡ç”¨
            self.sensitive_words = {
                'ç‹ç‹¸æ€§çˆ±é€šè®¯', 'äº²æ°‘å…š', 'è‹±è¯­æªæ‰‹', 'ä¸­å¤®æ— èƒ½', 'è€ƒè¯•æªæ‰‹',
                'ä¸­å›½å½“å±€', 'ä¼Šæ–¯å…°è¿åŠ¨', 'çŒ¥äºµ', 'å»ºå›½å…š', 'åä¸»å¸­'
            }

    def detect_sensitive_words(self, text):
        """æ£€æµ‹æ–‡æœ¬ä¸­çš„æ•æ„Ÿè¯"""
        if not text:
            return []

        text_lower = text.lower()
        detected_words = []

        for word in self.sensitive_words:
            if word in text_lower:
                detected_words.append(word)

        return detected_words

    def flat_map(self, value):
        """
        å¤„ç†æ¯æ¡è¯„è®ºæ•°æ®
        è¾“å…¥: JSONå­—ç¬¦ä¸²
        è¾“å‡º: å¤„ç†åçš„æ•°æ®(å¯èƒ½å¤šæ¡)
        """
        try:
            # è§£æJSONæ•°æ®
            comment_data = json.loads(value)

            # æå–è¯„è®ºå†…å®¹
            user_comment = comment_data.get('user_comment', '')
            order_id = comment_data.get('order_id', '')
            user_id = comment_data.get('user_id', '')

            # æ£€æµ‹æ•æ„Ÿè¯
            sensitive_words = self.detect_sensitive_words(user_comment)
            is_sensitive = len(sensitive_words) > 0

            # ä¸°å¯Œè¾“å‡ºæ•°æ®
            output_data = {
                **comment_data,
                'is_sensitive': is_sensitive,
                'sensitive_words': sensitive_words,
                'sensitive_word_count': len(sensitive_words),
                'process_timestamp': int(os.times().elapsed * 1000),  # å¤„ç†æ—¶é—´æˆ³
                'action_taken': 'blocked' if is_sensitive else 'passed'
            }

            # è¾“å‡ºå¤„ç†ç»“æœ
            yield json.dumps(output_data, ensure_ascii=False)

            # å¦‚æœæ˜¯æ•æ„Ÿè¯„è®ºï¼Œé¢å¤–ç”Ÿæˆå‘Šè­¦ä¿¡æ¯
            if is_sensitive:
                alert_data = {
                    'alert_type': 'SENSITIVE_COMMENT',
                    'order_id': order_id,
                    'user_id': user_id,
                    'sensitive_words': sensitive_words,
                    'comment_preview': user_comment[:100] + '...' if len(user_comment) > 100 else user_comment,
                    'alert_timestamp': int(os.times().elapsed * 1000),
                    'severity': 'HIGH'
                }
                yield json.dumps(alert_data, ensure_ascii=False)

        except Exception as e:
            logger.error(f"å¤„ç†è¯„è®ºæ•°æ®å¤±è´¥: {e}, åŸå§‹æ•°æ®: {value}")
            # è¾“å‡ºé”™è¯¯ä¿¡æ¯
            error_data = {
                'error': str(e),
                'raw_data': value,
                'process_timestamp': int(os.times().elapsed * 1000)
            }
            yield json.dumps(error_data, ensure_ascii=False)

class CommentDataGenerator:
    """
    è¯„è®ºæ•°æ®å¤„ç†ä¸»ç±»
    """

    def __init__(self):
        self.env = StreamExecutionEnvironment.get_execution_environment()
        # è®¾ç½®å¹¶è¡Œåº¦
        self.env.set_parallelism(1)

        # æ•°æ®åº“é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è·å–ï¼‰
        self.sqlserver_config = {
            'host': os.getenv("sqlserver_ip", "localhost"),
            'port': os.getenv("sqlserver_port", "1433"),
            'database': os.getenv("sqlserver_db", "realtime_v3"),
            'username': os.getenv("sqlserver_user_name", "sa"),
            'password': os.getenv("sqlserver_user_pwd", "password")
        }

        # Kafkaé…ç½®
        self.kafka_config = {
            'bootstrap_servers': os.getenv("kafka_bootstrap_servers", "cdh01:9092,cdh02:9092,cdh03:9092"),
            'source_topic': 'realtime_v3_logs',
            'sink_topic': 'processed_comments',
            'alert_topic': 'sensitive_alerts'
        }

    def create_kafka_source(self):
        """åˆ›å»ºKafkaæ•°æ®æº"""
        return KafkaSource.builder() \
            .set_bootstrap_servers(self.kafka_config['bootstrap_servers']) \
            .set_topics(self.kafka_config['source_topic']) \
            .set_group_id("flink_comment_processor") \
            .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \
            .set_value_only_deserializer(SimpleStringSchema()) \
            .build()

    def create_jdbc_sink(self):
        """åˆ›å»ºJDBC Sinkç”¨äºå†™å…¥é»‘åå•"""
        jdbc_connection_options = JdbcConnectionOptions.JdbcConnectionOptionsBuilder() \
            .with_url(f"jdbc:sqlserver://{self.sqlserver_config['host']}:{self.sqlserver_config['port']};"
                      f"databaseName={self.sqlserver_config['database']}") \
            .with_driver_name("com.microsoft.sqlserver.jdbc.SQLServerDriver") \
            .with_user_name(self.sqlserver_config['username']) \
            .with_password(self.sqlserver_config['password']) \
            .build()

        jdbc_execution_options = JdbcExecutionOptions.builder() \
            .with_batch_interval_ms(1000) \
            .with_batch_size(100) \
            .with_max_retries(3) \
            .build()

        return JdbcSink.sink(
            "INSERT INTO user_blacklist (user_id, order_id, sensitive_comment, sensitive_words, detected_time, ds, ts) VALUES (?, ?, ?, ?, ?, ?, ?)",
            self.create_statement_builder(),
            jdbc_connection_options,
            jdbc_execution_options
        )

    def create_statement_builder(self):
        """åˆ›å»ºJDBCè¯­å¥æ„å»ºå™¨"""
        from pyflink.datastream.connectors.jdbc import JdbcStatementBuilder

        class BlacklistStatementBuilder(JdbcStatementBuilder):
            def accept(self, statement, row):
                statement.setString(1, row[0])  # user_id
                statement.setString(2, row[1])  # order_id
                statement.setString(3, row[2])  # sensitive_comment
                statement.setString(4, row[3])  # sensitive_words
                statement.setLong(5, row[4])    # detected_time
                statement.setString(6, row[5])  # ds
                statement.setLong(7, row[6])    # ts

        return BlacklistStatementBuilder()

    def process_comments_to_blacklist(self, data_stream):
        """å¤„ç†æ•æ„Ÿè¯„è®ºåˆ°é»‘åå•"""
        from pyflink.common import Row

        def map_to_blacklist(comment_json):
            """å°†è¯„è®ºæ•°æ®æ˜ å°„ä¸ºé»‘åå•è®°å½•"""
            try:
                comment_data = json.loads(comment_json)

                if comment_data.get('is_sensitive', False):
                    # å‡†å¤‡é»‘åå•æ•°æ®
                    sensitive_comment = comment_data.get('user_comment', '')[:1000]  # é™åˆ¶é•¿åº¦
                    sensitive_words = ','.join(comment_data.get('sensitive_words', []))

                    return Row(
                        comment_data.get('user_id', ''),
                        comment_data.get('order_id', ''),
                        sensitive_comment,
                        sensitive_words,
                        comment_data.get('process_timestamp', 0),
                        comment_data.get('ds', ''),
                        comment_data.get('ts', 0)
                    )
                return None
            except Exception as e:
                logger.error(f"æ˜ å°„é»‘åå•æ•°æ®å¤±è´¥: {e}")
                return None

        # è¿‡æ»¤å¹¶æ˜ å°„æ•°æ®
        blacklist_stream = data_stream \
            .map(map_to_blacklist) \
            .filter(lambda x: x is not None)

        # å†™å…¥é»‘åå•è¡¨
        blacklist_stream.sink_to(self.create_jdbc_sink())

        return blacklist_stream

    def run(self):
        """è¿è¡ŒFlinkå¤„ç†ä½œä¸š"""
        logger.info("ğŸš€ å¯åŠ¨Flinkæ•æ„Ÿè¯„è®ºæ£€æµ‹ä½œä¸š...")

        try:
            # åˆ›å»ºæ•°æ®æº
            source = self.create_kafka_source()
            data_stream = self.env.from_source(
                source,
                WatermarkStrategy.no_watermarks(),
                "Kafka Source"
            )

            # æ•æ„Ÿè¯æ£€æµ‹å¤„ç†
            processed_stream = data_stream \
                .flat_map(SensitiveWordDetector()) \
                .name("SensitiveWordDetection")

            # è¾“å‡ºå¤„ç†ç»“æœåˆ°Kafkaï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦é…ç½®Kafka Sinkï¼‰
            processed_stream.print().name("ProcessedOutput")

            # å¤„ç†æ•æ„Ÿè¯„è®ºåˆ°é»‘åå•
            self.process_comments_to_blacklist(processed_stream)

            # æ‰§è¡Œä½œä¸š
            self.env.execute("Real-time Sensitive Comment Detection")

        except Exception as e:
            logger.error(f"Flinkä½œä¸šæ‰§è¡Œå¤±è´¥: {e}")
            raise

def create_blacklist_table_sql():
    """
    åˆ›å»ºé»‘åå•è¡¨çš„SQLè¯­å¥
    """
    return """
    IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='user_blacklist' AND xtype='U')
    CREATE TABLE user_blacklist (
        id BIGINT IDENTITY(1,1) PRIMARY KEY,
        user_id NVARCHAR(255) NOT NULL,
        order_id NVARCHAR(255) NOT NULL,
        sensitive_comment NVARCHAR(MAX),
        sensitive_words NVARCHAR(MAX),
        detected_time BIGINT,
        ds NVARCHAR(20),
        ts BIGINT,
        created_time DATETIME2 DEFAULT GETDATE(),
        status NVARCHAR(50) DEFAULT 'ACTIVE'
    );
    
    -- åˆ›å»ºç´¢å¼•
    IF NOT EXISTS (SELECT * FROM sys.indexes WHERE name='idx_user_blacklist_user_id')
    CREATE INDEX idx_user_blacklist_user_id ON user_blacklist(user_id);
    
    IF NOT EXISTS (SELECT * FROM sys.indexes WHERE name='idx_user_blacklist_order_id')  
    CREATE INDEX idx_user_blacklist_order_id ON user_blacklist(order_id);
    
    IF NOT EXISTS (SELECT * FROM sys.indexes WHERE name='idx_user_blacklist_created_time')
    CREATE INDEX idx_user_blacklist_created_time ON user_blacklist(created_time);
    """

def setup_database():
    """è®¾ç½®æ•°æ®åº“è¡¨"""
    import pymssql

    try:
        conn = pymssql.connect(
            server=os.getenv("sqlserver_ip"),
            port=os.getenv("sqlserver_port"),
            user=os.getenv("sqlserver_user_name"),
            password=os.getenv("sqlserver_user_pwd"),
            database=os.getenv("sqlserver_db")
        )

        with conn.cursor() as cursor:
            cursor.execute(create_blacklist_table_sql())
            conn.commit()
            logger.info("âœ… é»‘åå•è¡¨åˆ›å»º/éªŒè¯å®Œæˆ")

    except Exception as e:
        logger.error(f"æ•°æ®åº“è®¾ç½®å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    # è®¾ç½®æ•°æ®åº“
    setup_database()

    # å¯åŠ¨Flinkå¤„ç†ä½œä¸š
    processor = CommentDataGenerator()
    processor.run()